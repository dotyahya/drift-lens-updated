{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce77b14-7423-4809-8947-c9fa841199fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s289159/.conda/envs/airbnb-XAI-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertTokenizer,BertForSequenceClassification, BertConfig\n",
    "from transformers.pipelines import pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddbec06-2176-413a-aad9-bf9ad1f07412",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55020a9-eebd-41d8-b411-6d773f67289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"use_case/hf-bert-20-newsgroups-macroclass-drift-recreation-vldb\"\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(base_dir,\"dataset\",\"df_train_0-4.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(base_dir,\"dataset\",\"df_test_0-4.csv\"))\n",
    "df_new_unseen = pd.read_csv(os.path.join(base_dir,\"dataset\",\"df_new_unseen_0-4.csv\"))\n",
    "df_drifted = pd.read_csv(os.path.join(base_dir,\"dataset\",\"df_drifted_5.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b8c66c-f607-477e-882a-b68b114c81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"use_case/hf-bert-20-newsgroups-macroclass-drift-recreation-vldb/saved_model/best_model\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "BERT_MODEL = 'bert-base-uncased' # BERT model type\n",
    "\n",
    "config = BertConfig.from_pretrained(os.path.join(OUTPUT_DIR, CONFIG_NAME), output_hidden_states=True)\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_DIR), config=config)\n",
    "model = model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c32969-72ad-4144-a55f-62b221c27995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\"padding\":\"max_length\", \"truncation\":True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc8f4e7-8abb-4ea3-95a0-888b6e4072ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id2label = [\"Technology\", \"Sale-Ads\", \"Politics\", \"Religion\", \"Science\", \"Recreation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf9d4f0-8683-4761-9411-a5f67eb0eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding_and_predict(model, tokenizer, df):\n",
    "    \n",
    "    X = df[\"text\"].tolist() # List of input texts\n",
    "    Y_original_macro = df[\"macro_label_id\"].tolist() # List of original labels (GT)\n",
    "    Y_original_macro_names = [train_id2label[l] for l in Y_original_macro]  # List of original labels' names (GT)\n",
    "    \n",
    "    Y_original_micro = df[\"micro_label_id\"].tolist() # List of original labels (GT)\n",
    "    Y_original_micro_names = df[\"micro_label_name\"].tolist() # List of original labels (GT)\n",
    "    \n",
    "    E = np.empty((0,768)) # Initialize empty array of embeddings\n",
    "    Y_predicted = [] # Initialize empty list of predicted labels (IDs)\n",
    "    Y_predicted_names = [] # Initialize empty list of predicted labels (Names)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    n_batch = len(df)//BATCH_SIZE\n",
    "    remainer = len(df)%BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(n_batch)):\n",
    "        input_texts = df[\"text\"].iloc[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE].tolist()\n",
    "        \n",
    "        tokenized_texts = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_texts.to(device))\n",
    "            \n",
    "        batch_probabilities = nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
    "        batch_labels = torch.argmax(batch_probabilities, dim=1).tolist()\n",
    "\n",
    "        batch_probabilities_list = batch_probabilities.tolist()            \n",
    "        batch_labels_name = [train_id2label[l] for l in batch_labels] \n",
    "\n",
    "        Y_predicted.extend(batch_labels)\n",
    "        Y_predicted_names.extend(batch_labels_name)\n",
    "\n",
    "        last_layer_hidden_states_arr = outputs[\"hidden_states\"][12].detach().cpu().numpy()                   \n",
    "        embedding_CLS_arr = last_layer_hidden_states_arr[:, 0, :] # [BATCH_SIZE, 0 = CLS, 768]\n",
    "        E = np.vstack([E, embedding_CLS_arr])\n",
    "       \n",
    "    if remainer>0:\n",
    "        input_texts = df[\"text\"].iloc[-remainer:].tolist()\n",
    "\n",
    "        tokenized_texts = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_texts.to(device))\n",
    "\n",
    "        batch_probabilities = nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
    "        batch_labels = torch.argmax(batch_probabilities, dim=1).tolist()\n",
    "\n",
    "        batch_probabilities_list = batch_probabilities.tolist()            \n",
    "        batch_labels_name = [train_id2label[l] for l in batch_labels] \n",
    "\n",
    "        Y_predicted.extend(batch_labels)\n",
    "        Y_predicted_names.extend(batch_labels_name)\n",
    "\n",
    "        last_layer_hidden_states_arr = outputs[\"hidden_states\"][12].detach().cpu().numpy()                   \n",
    "        embedding_CLS_arr = last_layer_hidden_states_arr[:, 0, :] # [BATCH_SIZE, 0 = CLS, 768]\n",
    "        E = np.vstack([E, embedding_CLS_arr])\n",
    "        \n",
    "    return X, E, Y_original_macro, Y_original_micro, Y_original_macro_names, Y_original_micro_names, Y_predicted, Y_predicted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7df62e0-255e-4c5e-af16-847e2c1f4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:50<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, E_test, Y_original_test_macro, Y_original_test_micro, Y_original_names_test_macro, Y_original_names_test_micro, Y_predicted_test, Y_predicted_names_test = extract_embedding_and_predict(model, tokenizer, df_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701b1c63-cf2f-4b34-8fc4-017fe11a90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [01:14<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, E_train, Y_original_train_macro, Y_original_train_micro, Y_original_names_train_macro, Y_original_names_train_micro, Y_predicted_train, Y_predicted_names_train = extract_embedding_and_predict(model, tokenizer, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d0900c-4589-4a10-8a07-ae08a8be619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:46<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "X_drift, E_drift, Y_original_drift_macro, Y_original_drift_micro, Y_original_names_drift_macro, Y_original_names_drift_micro, Y_predicted_drift, Y_predicted_names_drift = extract_embedding_and_predict(model, tokenizer, df_drifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39157c46-3728-4223-87ce-a96612221846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 98/173 [00:48<00:34,  2.15it/s]"
     ]
    }
   ],
   "source": [
    "X_new_unseen, E_new_unseen, Y_original_new_unseen_macro, Y_original_new_unseen_micro, Y_original_names_new_unseen_macro, Y_original_names_new_unseen_micro, Y_predicted_new_unseen, Y_predicted_names_new_unseen = extract_embedding_and_predict(model, tokenizer, df_new_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a774c00-1e89-46af-9e46-3f3b2beacd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0c8ff-a57a-4b85-b64f-e2a03a9177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(output_path, X, E, Y_original_macro, Y_original_micro, Y_original_names_macro, Y_original_names_micro, Y_predicted, Y_predicted_names):\n",
    "\n",
    "    fp = h5py.File(output_path, \"w\")\n",
    "\n",
    "    fp.create_dataset(\"X\", data=X, compression=\"gzip\")\n",
    "    fp.create_dataset(\"E\", data=E, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original\", data=Y_original_macro, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_names\", data=Y_original_names_macro, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_macro\", data=Y_original_macro, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_names_macro\", data=Y_original_names_macro, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_micro\", data=Y_original_macro, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_names_micro\", data=Y_original_names_macro, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_predicted\", data=Y_predicted, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_predicted_names\", data=Y_predicted_names, compression=\"gzip\")\n",
    "    fp.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927406b-ca13-49e1-bbe3-8aa75d011878",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir = os.path.join(base_dir, \"saved_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad112a7-9503-43a7-a7f2-98d0ab7b5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"train_embedding_0-4.hdf5\"), \n",
    "                X_train, \n",
    "                E_train, \n",
    "                Y_original_train_macro, \n",
    "               Y_original_train_micro,\n",
    "                Y_original_names_train_macro, \n",
    "               Y_original_names_train_micro,\n",
    "                Y_predicted_train, \n",
    "                Y_predicted_names_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b3385-fec1-45e6-8a62-4fa90633b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"test_embedding_0-4.hdf5\"), \n",
    "                X_test, \n",
    "                E_test, \n",
    "                Y_original_test_macro, \n",
    "               Y_original_test_micro,\n",
    "                Y_original_names_test_macro, \n",
    "               Y_original_names_test_micro,\n",
    "                Y_predicted_test, \n",
    "                Y_predicted_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e583bbe-2598-434a-81af-f598e56f9d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"new_unseen_embedding_0-4.hdf5\"), \n",
    "                X_new_unseen, \n",
    "                E_new_unseen, \n",
    "                Y_original_new_unseen_macro, \n",
    "               Y_original_new_unseen_micro,\n",
    "                Y_original_names_new_unseen_macro, \n",
    "               Y_original_names_new_unseen_micro,\n",
    "                Y_predicted_new_unseen, \n",
    "                Y_predicted_names_new_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2356f6-3ebd-43a7-99cf-acd4852021fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"drifted_embedding_5.hdf5\"),\n",
    "                X_drift, \n",
    "                E_drift, \n",
    "                Y_original_drift_macro, \n",
    "               Y_original_drift_micro,\n",
    "                Y_original_names_drift_macro, \n",
    "               Y_original_names_drift_micro,\n",
    "                Y_predicted_drift, \n",
    "                Y_predicted_names_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b4fc7-0e0b-4849-8d54-9c708abcae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb-XAI-env",
   "language": "python",
   "name": "airbnb-xai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}