{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d48f47-c110-4539-af6b-6de4b71b9855",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s289159/.conda/envs/audio-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12091766-8c79-4548-9be7-43347cc72203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"saved_model/best_model_wav2vec_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09effea-12c2-47d8-bfe0-4a8ac644ca0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d062ec-08fd-4204-a790-b752a270aeaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"audio-classification\", model=model_path, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6ac801-f7e7-431f-b5f0-8381531eb373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = load_from_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30bb82f-27c8-44f6-83c8-c7d6d2cc2046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 70578\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 9951\n",
       "    })\n",
       "    new_unseen: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 29556\n",
       "    })\n",
       "    drift: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 42697\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45679cd2-00b5-45e6-ad3b-d44afaf34d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def convert_label(example):\n",
    "    example['gender'] = int(label2id[example['gender']])\n",
    "    return example\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59908520-9c16-4202-a006-57afc60b6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"male\", \"female\"]\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502087a2-674f-40d9-ab27-618cd2b696e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets['train']\n",
    "test_dataset = datasets['test']\n",
    "new_unseen_dataset = datasets['new_unseen']\n",
    "drift_dataset = datasets['drift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d4c7ee-c260-4e7a-9964-8be0ad4abf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "new_unseen_dataset = new_unseen_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "drift_dataset = drift_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d499a9-bc25-4a5a-85a5-b1eb9e85273f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70578/70578 [00:16<00:00, 4400.32ex/s]\n",
      "100%|██████████| 9951/9951 [00:02<00:00, 4216.79ex/s]\n",
      "100%|██████████| 29556/29556 [00:06<00:00, 4465.86ex/s]\n",
      "100%|██████████| 42697/42697 [00:09<00:00, 4487.51ex/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(convert_label)\n",
    "test_dataset = test_dataset.map(convert_label)\n",
    "new_unseen_dataset = new_unseen_dataset.map(convert_label)\n",
    "drift_dataset = drift_dataset.map(convert_label)\n",
    "\n",
    "#encoded_train_audios = train_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "#encoded_train_audios = encoded_train_audios.rename_column(\"gender\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ab80e3-cb2f-46ee-950b-0e1e3e18efb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089c1223-c62b-40c7-987c-d13064aeb283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at saved_model/best_model_wav2vec_base were not used when initializing Wav2Vec2Model: ['classifier.weight', 'projector.weight', 'classifier.bias', 'projector.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28165722-b354-4cd6-9ae5-000aed6ed409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (2): Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (3): Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (4): Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5): Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (6): Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697bec8-d41c-4feb-a29a-37f6b823625e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#inputs = feature_extractor(datasets[\"test\"][0][\"audio\"][\"array\"], sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cb850-6f70-4ca7-a110-86abbf1697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = feature_extractor(datasets[\"test\"][0][\"audio\"][\"array\"], sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea70dea-d573-418d-8568-009d74361087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_path, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55650c-96eb-4045-9528-0ae490ee3e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encoded_train_audios[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf5b714-bfad-4d30-b41e-85babb2e1e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def extract_embedding(dataset):\n",
    "    original_label_ids = []\n",
    "    predicted_label_ids = []\n",
    "    \n",
    "    original_label_names = []\n",
    "    predicted_label_names = []\n",
    "    \n",
    "    last_hidden_states = []\n",
    "    \n",
    "    accents = []\n",
    "    \n",
    "    for sample in tqdm(dataset):\n",
    "        inputs = feature_extractor(sample[\"audio\"][\"array\"], sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_hidden_state = hidden_states[-1]\n",
    "        averaged_hidden_state = torch.mean(last_hidden_state, dim=1)\n",
    "        averaged_hidden_state_np = averaged_hidden_state.numpy()\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predicted_label_id = torch.argmax(logits).item()\n",
    "        predicted_label = model.config.id2label[predicted_label_id]\n",
    "        predicted_label_id = int(predicted_label_id)\n",
    "        \n",
    "        original_label_id = sample['gender']\n",
    "        original_label = model.config.id2label[original_label_id]\n",
    "        \n",
    "        \n",
    "        original_label_ids.append(original_label_id)\n",
    "        predicted_label_ids.append(predicted_label_id)\n",
    "        original_label_names.append(original_label)\n",
    "        predicted_label_names.append(predicted_label)\n",
    "        \n",
    "        last_hidden_states.append(averaged_hidden_state_np.squeeze(0))\n",
    "        \n",
    "        accents.append(sample['accent'])\n",
    "        \n",
    "    embedding_matrix = np.vstack(last_hidden_states)\n",
    "        \n",
    "    return embedding_matrix, original_label_ids, original_label_names, predicted_label_ids, predicted_label_names, accents\n",
    "\n",
    "\n",
    "def save_embedding(output_path, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names, accents):\n",
    "\n",
    "    fp = h5py.File(output_path, \"w\")\n",
    "    fp.create_dataset(\"E\", data=E, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original\", data=Y_original, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_names\", data=Y_original_names, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_predicted\", data=Y_predicted, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_predicted_names\", data=Y_predicted_names, compression=\"gzip\")\n",
    "    fp.create_dataset(\"accents\", data=accents, compression=\"gzip\")\n",
    "    fp.close()\n",
    "    return\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1896cc6a-3adb-4d6a-b8b6-95fabb0d6206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dir = os.path.join(\"saved_embedding\", \"gender_classification\", \"wav2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e1cb6-6a73-4c81-a20b-001c82f3c1c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2861/70578 [05:56<2:29:10,  7.57it/s]"
     ]
    }
   ],
   "source": [
    "E_train, Y_original_train, Y_original_names_train, Y_predicted_train, Y_predicted_names_train, accents_train = extract_embedding(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc9d6d-cca5-489d-9b91-15f784087cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"train_embedding.hdf5\"), \n",
    "                E_train, \n",
    "                Y_original_train, \n",
    "                Y_original_names_train, \n",
    "                Y_predicted_train, \n",
    "                Y_predicted_names_train,\n",
    "                accents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63bdaa-6572-48be-a3c7-e5086f58bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_test, Y_original_test, Y_original_names_test, Y_predicted_test, Y_predicted_names_test, accents_test = extract_embedding(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e45323-9c33-4be5-b15e-306ae999dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"test_embedding.hdf5\"), \n",
    "                E_test, \n",
    "                Y_original_test, \n",
    "                Y_original_names_test, \n",
    "                Y_predicted_test, \n",
    "                Y_predicted_names_test,\n",
    "                accents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1dfcdd-8e6f-4e0a-bdc5-2f0a45945382",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_new_unseen, Y_original_new_unseen, Y_original_names_new_unseen, Y_predicted_new_unseen, Y_predicted_names_new_unseen, accents_new_unseen = extract_embedding(new_unseen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11a311-50e9-4f12-9fc8-5af4aee8b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"new_unseen_embedding.hdf5\"), \n",
    "                E_new_unseen, \n",
    "                Y_original_new_unseen, \n",
    "                Y_original_names_new_unseen, \n",
    "                Y_predicted_new_unseen, \n",
    "                Y_predicted_names_new_unseen,\n",
    "                accents_new_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81751d3-1041-4fec-a6e6-7e881c2201bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_drift, Y_original_drift, Y_original_names_drift, Y_predicted_drift, Y_predicted_names_drift, accents_drift = extract_embedding(drift_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a1f4c-81d6-492c-9d08-747b97ff436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"drift_embedding.hdf5\"), \n",
    "                E_drift, \n",
    "                Y_original_drift, \n",
    "                Y_original_names_drift, \n",
    "                Y_predicted_drift, \n",
    "                Y_predicted_names_drift,\n",
    "                accents_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3e2b4-1db7-4993-8279-963a8e13c599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-env",
   "language": "python",
   "name": "audio-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
