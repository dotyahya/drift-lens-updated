{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e3e33a-871e-437d-a073-eb19631e214c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s289159/.conda/envs/audio-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f783dd-d086-480b-be23-fb7e5b252fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-base\"\n",
    "#model_name = \"facebook/wav2vec2-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e703ba-d1a5-4935-a555-efd5b13c269e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s289159/.conda/envs/audio-env/lib/python3.9/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7e6ea2-ec80-4e88-aedc-285fd7ae4f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = load_from_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12cb2f25-c543-4c9b-a332-1e0af5828a98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 70578\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 9951\n",
       "    })\n",
       "    new_unseen: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 29556\n",
       "    })\n",
       "    drift: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 42697\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c2abe9-8058-4333-b1aa-db5a3765ece1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"male\", \"female\"]\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142f000b-5a7f-45e0-be7b-36adcb165359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': '0', 'female': '1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42f7ef9-60c4-489b-8f0d-02270d6b3a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'male', '1': 'female'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ebd37a1-f66b-429c-b0f3-869f9314fb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets['train']\n",
    "test_dataset = datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061dfbf7-6e38-4394-8d39-f5fbc3aa4ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '29b1e5a58d1667d4ac45832ec195356598a69f66680877b0d5ee465ce2404c0186affc81ffe4a29df35203fc07a0fc5714c60d914a88aa36d7f84c94dc381d2f',\n",
       " 'path': '/home/students/s289159/.cache/huggingface/datasets/downloads/extracted/bab7205fb7eb744fb5f8ca6c7fc1a1096a0632122d952545fad5b84bb21ff6e4/cv-corpus-6.1-2020-12-11/en/clips/common_voice_en_122577.mp3',\n",
       " 'audio': {'path': '/home/students/s289159/.cache/huggingface/datasets/downloads/extracted/bab7205fb7eb744fb5f8ca6c7fc1a1096a0632122d952545fad5b84bb21ff6e4/cv-corpus-6.1-2020-12-11/en/clips/common_voice_en_122577.mp3',\n",
       "  'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00863345,\n",
       "          0.00414815, -0.00097276], dtype=float32),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': 'Two women are smiling next to a microphone on a stage.',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': 'teens',\n",
       " 'gender': 'male',\n",
       " 'accent': 'us',\n",
       " 'locale': 'en',\n",
       " 'segment': \"''\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af10ee16-fe73-4799-bfbd-286f1ab850e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 'f148bbf4cd30561010300193263d00b4b009118933da4c5cc7c8cb166f24e9a1cd232f8073c7574055f8dbb373fb0d69b28b5f5e9659d011feff4345e160044f',\n",
       " 'path': '/home/students/s289159/.cache/huggingface/datasets/downloads/extracted/bab7205fb7eb744fb5f8ca6c7fc1a1096a0632122d952545fad5b84bb21ff6e4/cv-corpus-6.1-2020-12-11/en/clips/common_voice_en_162540.mp3',\n",
       " 'audio': {'path': '/home/students/s289159/.cache/huggingface/datasets/downloads/extracted/bab7205fb7eb744fb5f8ca6c7fc1a1096a0632122d952545fad5b84bb21ff6e4/cv-corpus-6.1-2020-12-11/en/clips/common_voice_en_162540.mp3',\n",
       "  'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "         -8.9534951e-06,  5.4251259e-06, -2.7791944e-05], dtype=float32),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': 'Two young, White males are outside near many bushes.',\n",
       " 'up_votes': 3,\n",
       " 'down_votes': 0,\n",
       " 'age': 'seventies',\n",
       " 'gender': 'male',\n",
       " 'accent': 'us',\n",
       " 'locale': 'en',\n",
       " 'segment': \"''\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8af0a44-3598-4a42-9561-1ad6ade9fd64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def convert_label(example):\n",
    "    example['gender'] = int(label2id[example['gender']])\n",
    "    return example\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754db782-aeda-4d74-a6aa-5cd4c5d46b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88edc03-95b1-4072-8da1-967872d4d0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male',\n",
       " 'male']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['gender'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be836a78-4c2b-4ea4-929c-bef90550d1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "test_dataset = test_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a88c721-c71c-4dae-8978-ea0f744cc9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70578/70578 [00:17<00:00, 3958.81ex/s]\n",
      "100%|██████████| 71/71 [22:35<00:00, 19.09s/ba]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(convert_label)\n",
    "encoded_train_audios = train_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_train_audios = encoded_train_audios.rename_column(\"gender\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac33e19-f509-4f55-af81-351adf9a66d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9951/9951 [00:02<00:00, 4116.63ex/s]\n",
      "100%|██████████| 10/10 [03:06<00:00, 18.62s/ba]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(convert_label)\n",
    "encoded_test_audios = test_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_test_audios = encoded_test_audios.rename_column(\"gender\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49da0df0-5b19-4619-a9af-7c93b7e39d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'label', 'accent', 'locale', 'segment', 'input_values'],\n",
       "    num_rows: 70578\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40528224-7f13-4149-a18f-4f31e25d6054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_audios['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d79da0c8-daf7-4896-98a8-1b9f6cb41e69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'label', 'accent', 'locale', 'segment', 'input_values'],\n",
       "    num_rows: 9951\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "922d461d-092e-4b26-843d-0a023661c2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_audios['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c53e594-3776-464d-8156-8673c440b5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5efe0913-83ff-42bc-81f6-0b33a5b3c5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s289159/.conda/envs/audio-env/lib/python3.9/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias', 'project_hid.bias', 'project_hid.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.bias', 'classifier.weight', 'projector.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_name,\n",
    "                                                        num_labels=num_labels, \n",
    "                                                        label2id=label2id, \n",
    "                                                        id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b997c268-8090-4294-b704-ed9d9871f02a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import sklearn\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    print(pred)\n",
    "    try:\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "    except:\n",
    "        preds = pred.predictions[0].argmax(-1)\n",
    "    precision, recall, f1, _ = sklearn.metrics.precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", labels=list(set(labels))\n",
    "    )\n",
    "    print(sklearn.metrics.classification_report(labels, preds, digits=4))\n",
    "    acc = sklearn.metrics.accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27aae799-3cf3-42ef-b57e-eac34a1196f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #learning_rate=3e-5,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_audios,\n",
    "    eval_dataset=encoded_test_audios,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5ae31c7-a63f-4e52-a12b-434c2ce92762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/students/s289159/.conda/envs/audio-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 70578\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 44120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44120' max='44120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44120/44120 3:38:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.498229</td>\n",
       "      <td>0.819315</td>\n",
       "      <td>0.819307</td>\n",
       "      <td>0.819320</td>\n",
       "      <td>0.819302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.406211</td>\n",
       "      <td>0.843935</td>\n",
       "      <td>0.841268</td>\n",
       "      <td>0.867323</td>\n",
       "      <td>0.843415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.696439</td>\n",
       "      <td>0.502261</td>\n",
       "      <td>0.334875</td>\n",
       "      <td>0.626081</td>\n",
       "      <td>0.500203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.643022</td>\n",
       "      <td>0.776404</td>\n",
       "      <td>0.770976</td>\n",
       "      <td>0.806931</td>\n",
       "      <td>0.777052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.540030</td>\n",
       "      <td>0.835192</td>\n",
       "      <td>0.834859</td>\n",
       "      <td>0.838373</td>\n",
       "      <td>0.835392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.498012</td>\n",
       "      <td>0.860919</td>\n",
       "      <td>0.860827</td>\n",
       "      <td>0.861625</td>\n",
       "      <td>0.860828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.822631</td>\n",
       "      <td>0.821148</td>\n",
       "      <td>0.834617</td>\n",
       "      <td>0.823020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.608336</td>\n",
       "      <td>0.842126</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>0.846990</td>\n",
       "      <td>0.842370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.651757</td>\n",
       "      <td>0.843131</td>\n",
       "      <td>0.842626</td>\n",
       "      <td>0.848178</td>\n",
       "      <td>0.843379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.703203</td>\n",
       "      <td>0.849965</td>\n",
       "      <td>0.849439</td>\n",
       "      <td>0.855535</td>\n",
       "      <td>0.850223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0f1550>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8200    0.8163    0.8182      4955\n",
      "           1     0.8187    0.8223    0.8205      4996\n",
      "\n",
      "    accuracy                         0.8193      9951\n",
      "   macro avg     0.8193    0.8193    0.8193      9951\n",
      "weighted avg     0.8193    0.8193    0.8193      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-4412\n",
      "Configuration saved in ./results/checkpoint-4412/config.json\n",
      "Model weights saved in ./results/checkpoint-4412/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-4412/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e4bc4c0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9590    0.7173    0.8207      4955\n",
      "           1     0.7757    0.9696    0.8618      4996\n",
      "\n",
      "    accuracy                         0.8439      9951\n",
      "   macro avg     0.8673    0.8434    0.8413      9951\n",
      "weighted avg     0.8669    0.8439    0.8414      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-8824\n",
      "Configuration saved in ./results/checkpoint-8824/config.json\n",
      "Model weights saved in ./results/checkpoint-8824/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-8824/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0efbe0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.0006    0.0012      4955\n",
      "           1     0.5022    0.9998    0.6685      4996\n",
      "\n",
      "    accuracy                         0.5023      9951\n",
      "   macro avg     0.6261    0.5002    0.3349      9951\n",
      "weighted avg     0.6256    0.5023    0.3362      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-13236\n",
      "Configuration saved in ./results/checkpoint-13236/config.json\n",
      "Model weights saved in ./results/checkpoint-13236/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-13236/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0d8fa0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7091    0.9342    0.8062      4955\n",
      "           1     0.9048    0.6199    0.7357      4996\n",
      "\n",
      "    accuracy                         0.7764      9951\n",
      "   macro avg     0.8069    0.7771    0.7710      9951\n",
      "weighted avg     0.8073    0.7764    0.7708      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-17648\n",
      "Configuration saved in ./results/checkpoint-17648/config.json\n",
      "Model weights saved in ./results/checkpoint-17648/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-17648/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0e69d0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8045    0.8838    0.8423      4955\n",
      "           1     0.8722    0.7870    0.8274      4996\n",
      "\n",
      "    accuracy                         0.8352      9951\n",
      "   macro avg     0.8384    0.8354    0.8349      9951\n",
      "weighted avg     0.8385    0.8352    0.8348      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-22060\n",
      "Configuration saved in ./results/checkpoint-22060/config.json\n",
      "Model weights saved in ./results/checkpoint-22060/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-22060/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f6634335d00>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8766    0.8387    0.8573      4955\n",
      "           1     0.8466    0.8829    0.8644      4996\n",
      "\n",
      "    accuracy                         0.8609      9951\n",
      "   macro avg     0.8616    0.8608    0.8608      9951\n",
      "weighted avg     0.8616    0.8609    0.8608      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-26472\n",
      "Configuration saved in ./results/checkpoint-26472/config.json\n",
      "Model weights saved in ./results/checkpoint-26472/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-26472/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0efbe0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7702    0.9175    0.8374      4955\n",
      "           1     0.8990    0.7286    0.8049      4996\n",
      "\n",
      "    accuracy                         0.8226      9951\n",
      "   macro avg     0.8346    0.8230    0.8211      9951\n",
      "weighted avg     0.8349    0.8226    0.8211      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-30884\n",
      "Configuration saved in ./results/checkpoint-30884/config.json\n",
      "Model weights saved in ./results/checkpoint-30884/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-30884/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f6634335f70>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8049    0.9015    0.8505      4955\n",
      "           1     0.8891    0.7832    0.8328      4996\n",
      "\n",
      "    accuracy                         0.8421      9951\n",
      "   macro avg     0.8470    0.8424    0.8416      9951\n",
      "weighted avg     0.8472    0.8421    0.8416      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-35296\n",
      "Configuration saved in ./results/checkpoint-35296/config.json\n",
      "Model weights saved in ./results/checkpoint-35296/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-35296/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0ef9a0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8052    0.9035    0.8515      4955\n",
      "           1     0.8911    0.7832    0.8337      4996\n",
      "\n",
      "    accuracy                         0.8431      9951\n",
      "   macro avg     0.8482    0.8434    0.8426      9951\n",
      "weighted avg     0.8484    0.8431    0.8426      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-39708\n",
      "Configuration saved in ./results/checkpoint-39708/config.json\n",
      "Model weights saved in ./results/checkpoint-39708/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-39708/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f6634335f70>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8100    0.9128    0.8583      4955\n",
      "           1     0.9011    0.7876    0.8405      4996\n",
      "\n",
      "    accuracy                         0.8500      9951\n",
      "   macro avg     0.8555    0.8502    0.8494      9951\n",
      "weighted avg     0.8557    0.8500    0.8494      9951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-44120\n",
      "Configuration saved in ./results/checkpoint-44120/config.json\n",
      "Model weights saved in ./results/checkpoint-44120/pytorch_model.bin\n",
      "Feature extractor saved in ./results/checkpoint-44120/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-26472 (score: 0.8608271100478655).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=44120, training_loss=0.23970040963761718, metrics={'train_runtime': 13093.2625, 'train_samples_per_second': 53.904, 'train_steps_per_second': 3.37, 'total_flos': 6.4075173446592e+18, 'train_loss': 0.23970040963761718, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "494e4c5f-7c02-4b3a-b8c6-4ac8735342f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes. If client_id, down_votes, path, age, segment, sentence, accent, locale, up_votes are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9951\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION BEST MODEL ON TEST SET\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='622' max='622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [622/622 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7f662e0efbe0>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8766    0.8387    0.8573      4955\n",
      "           1     0.8466    0.8829    0.8644      4996\n",
      "\n",
      "    accuracy                         0.8609      9951\n",
      "   macro avg     0.8616    0.8608    0.8608      9951\n",
      "weighted avg     0.8616    0.8609    0.8608      9951\n",
      "\n",
      "{'eval_loss': 0.49801209568977356, 'eval_accuracy': 0.8609185006532006, 'eval_f1': 0.8608271100478655, 'eval_precision': 0.8616246926695332, 'eval_recall': 0.8608275318539393, 'eval_runtime': 73.831, 'eval_samples_per_second': 134.781, 'eval_steps_per_second': 8.425, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"EVALUATION BEST MODEL ON TEST SET\")\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a28ac0e-2f34-4999-a239-c85216ebb464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_model/best_model_wav2vec_base\n",
      "Configuration saved in saved_model/best_model_wav2vec_base/config.json\n",
      "Model weights saved in saved_model/best_model_wav2vec_base/pytorch_model.bin\n",
      "Feature extractor saved in saved_model/best_model_wav2vec_base/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "trainer.save_model(os.path.join(\"saved_model\", \"best_model_wav2vec_base\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850db55-ea12-4aaa-8490-b31507c0ba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-env",
   "language": "python",
   "name": "audio-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
