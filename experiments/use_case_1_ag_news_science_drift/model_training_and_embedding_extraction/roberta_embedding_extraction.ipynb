{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce77b14-7423-4809-8947-c9fa841199fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/s289159/.conda/envs/audio-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import RobertaTokenizer,RobertaForSequenceClassification, RobertaConfig\n",
    "from transformers.pipelines import pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddbec06-2176-413a-aad9-bf9ad1f07412",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55020a9-eebd-41d8-b411-6d773f67289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"static/data/roberta\"\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(data_dir, \"df_train_0_1_2.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(data_dir, \"df_test_0_1_2.csv\"))\n",
    "df_new_unseen = pd.read_csv(os.path.join(data_dir, \"df_new_unseen_0_1_2.csv\"))\n",
    "df_drifted = pd.read_csv(os.path.join(data_dir, \"df_drifted_3.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b8c66c-f607-477e-882a-b68b114c81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"static/saved_models/roberta/best_model\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "BERT_MODEL = 'roberta-base' # BERT model type\n",
    "\n",
    "config = RobertaConfig.from_pretrained(os.path.join(MODEL_DIR, CONFIG_NAME), output_hidden_states=True)\n",
    "model = RobertaForSequenceClassification.from_pretrained(os.path.join(MODEL_DIR), config=config)\n",
    "model = model.to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c32969-72ad-4144-a55f-62b221c27995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\"padding\":\"max_length\", \"truncation\":True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efc8f4e7-8abb-4ea3-95a0-888b6e4072ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id2label = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf9d4f0-8683-4761-9411-a5f67eb0eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding_and_predict(model, tokenizer, df):\n",
    "    \n",
    "    X = df[\"text\"].tolist() # List of input texts\n",
    "    Y_original = df[\"label\"].tolist() # List of original labels (GT)\n",
    "    Y_original_names = [train_id2label[l] for l in Y_original]  # List of original labels' names (GT)\n",
    "    E = np.empty((0,768)) # Initialize empty array of embeddings\n",
    "    Y_predicted = [] # Initialize empty list of predicted labels (IDs)\n",
    "    Y_predicted_names = [] # Initialize empty list of predicted labels (Names)\n",
    "    \n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "    n_batch = len(df)//BATCH_SIZE\n",
    "    remainer = len(df)%BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(n_batch)):\n",
    "        input_texts = df[\"text\"].iloc[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE].tolist()\n",
    "        \n",
    "        tokenized_texts = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_texts.to(device))\n",
    "            \n",
    "        batch_probabilities = nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
    "        batch_labels = torch.argmax(batch_probabilities, dim=1).tolist()\n",
    "\n",
    "        batch_probabilities_list = batch_probabilities.tolist()            \n",
    "        batch_labels_name = [train_id2label[l] for l in batch_labels] \n",
    "\n",
    "        Y_predicted.extend(batch_labels)\n",
    "        Y_predicted_names.extend(batch_labels_name)\n",
    "\n",
    "        last_layer_hidden_states_arr = outputs[\"hidden_states\"][12].detach().cpu().numpy()                   \n",
    "        embedding_CLS_arr = last_layer_hidden_states_arr[:, 0, :] # [BATCH_SIZE, 0 = CLS, 768]\n",
    "        E = np.vstack([E, embedding_CLS_arr])\n",
    "            \n",
    "    input_texts = df[\"text\"].iloc[-remainer:].tolist()\n",
    "        \n",
    "    tokenized_texts = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_texts.to(device))\n",
    "\n",
    "    batch_probabilities = nn.functional.softmax(outputs[\"logits\"], dim=-1)\n",
    "    batch_labels = torch.argmax(batch_probabilities, dim=1).tolist()\n",
    "\n",
    "    batch_probabilities_list = batch_probabilities.tolist()            \n",
    "    batch_labels_name = [train_id2label[l] for l in batch_labels] \n",
    "\n",
    "    Y_predicted.extend(batch_labels)\n",
    "    Y_predicted_names.extend(batch_labels_name)\n",
    "\n",
    "    last_layer_hidden_states_arr = outputs[\"hidden_states\"][12].detach().cpu().numpy()                   \n",
    "    embedding_CLS_arr = last_layer_hidden_states_arr[:, 0, :] # [BATCH_SIZE, 0 = CLS, 768]\n",
    "    E = np.vstack([E, embedding_CLS_arr])\n",
    "        \n",
    "    return X, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7df62e0-255e-4c5e-af16-847e2c1f4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:14<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, E_test, Y_original_test, Y_original_names_test, Y_predicted_test, Y_predicted_names_test = extract_embedding_and_predict(model, tokenizer, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "701b1c63-cf2f-4b34-8fc4-017fe11a90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [02:57<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, E_train, Y_original_train, Y_original_names_train, Y_predicted_train, Y_predicted_names_train = extract_embedding_and_predict(model, tokenizer, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11d0900c-4589-4a10-8a07-ae08a8be619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [01:52<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "X_drift, E_drift, Y_original_drift, Y_original_names_drift, Y_predicted_drift, Y_predicted_names_drift = extract_embedding_and_predict(model, tokenizer, df_drifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39157c46-3728-4223-87ce-a96612221846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [01:25<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "X_new_unseen, E_new_unseen, Y_original_new_unseen, Y_original_names_new_unseen, Y_predicted_new_unseen, Y_predicted_names_new_unseen = extract_embedding_and_predict(model, tokenizer, df_new_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a774c00-1e89-46af-9e46-3f3b2beacd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9da0c8ff-a57a-4b85-b64f-e2a03a9177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(output_path, X, E, Y_original, Y_original_names, Y_predicted, Y_predicted_names):\n",
    "\n",
    "    fp = h5py.File(output_path, \"w\")\n",
    "\n",
    "    fp.create_dataset(\"X\", data=X, compression=\"gzip\")\n",
    "    fp.create_dataset(\"E\", data=E, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original\", data=Y_original, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_original_names\", data=Y_original_names, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_predicted\", data=Y_predicted, compression=\"gzip\")\n",
    "    fp.create_dataset(\"Y_predicted_names\", data=Y_predicted_names, compression=\"gzip\")\n",
    "    fp.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9927406b-ca13-49e1-bbe3-8aa75d011878",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir = os.path.join(\"static\", \"saved_embeddings\", \"roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ad112a7-9503-43a7-a7f2-98d0ab7b5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"train_embedding_0_1_2.hdf5\"), \n",
    "                X_train, \n",
    "                E_train, \n",
    "                Y_original_train, \n",
    "                Y_original_names_train, \n",
    "                Y_predicted_train, \n",
    "                Y_predicted_names_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f8b3385-fec1-45e6-8a62-4fa90633b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"test_embedding_0_1_2.hdf5\"), \n",
    "                X_test, \n",
    "                E_test, \n",
    "                Y_original_test, \n",
    "                Y_original_names_test, \n",
    "                Y_predicted_test, \n",
    "                Y_predicted_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c2356f6-3ebd-43a7-99cf-acd4852021fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"drifted_embedding_3.hdf5\"), \n",
    "                X_drift, \n",
    "                E_drift, \n",
    "                Y_original_drift, \n",
    "                Y_original_names_drift, \n",
    "                Y_predicted_drift, \n",
    "                Y_predicted_names_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31394014-b40e-4738-b284-21a90600f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(os.path.join(embedding_dir, \"new_unseen_embedding_0_1_2.hdf5\"), \n",
    "                X_new_unseen, \n",
    "                E_new_unseen, \n",
    "                Y_original_new_unseen, \n",
    "                Y_original_names_new_unseen, \n",
    "                Y_predicted_new_unseen, \n",
    "                Y_predicted_names_new_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b4fc7-0e0b-4849-8d54-9c708abcae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-env",
   "language": "python",
   "name": "audio-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}